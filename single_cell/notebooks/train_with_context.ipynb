{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tiff\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from src.dataset_CELTIC import CELTICDataGen\n",
    "from src.models.MaskGenerator_CELTIC import MaskGenerator\n",
    "from src.models.UNETO_CELTIC import UNet3D\n",
    "from src.models.fnet_model import CELTICModel\n",
    "from src.transforms import normalize, normalize_with_mask, Propper\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CELTICWrapper(nn.Module):\n",
    "    def __init__(self, celtic_model):\n",
    "        super().__init__()\n",
    "        self.model = celtic_model\n",
    "        self.model.net.eval()\n",
    "        for param in self.model.net.parameters():\n",
    "            param.requires_grad = False  # Freeze weights\n",
    "\n",
    "    def forward(self, signal: torch.Tensor, context: torch.Tensor = None, m = None):\n",
    "        # Assumes signal is (1, C, D, H, W), context is (1, F)\n",
    "        # with torch.no_grad():\n",
    "        #     if context is not None:\n",
    "        #         return self.model.net(signal, context)\n",
    "        #     else:\n",
    "        #         return self.model.net(signal)\n",
    "\n",
    "        if context is not None:\n",
    "            pred = self.model.net(signal, context)\n",
    "            pred = pred * m\n",
    "        else:\n",
    "            pred = self.model.net(signal)\n",
    "            pred = pred * m\n",
    "        return pred\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define base path for all operations\n",
    "BASE_PATH = os.path.dirname(os.getcwd())\n",
    "print(BASE_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "CONTINUE_TRAINING = True\n",
    "weighted_pcc = False\n",
    "signals_are_masked = True\n",
    "organelle = \"Nuclear-envelope\"\n",
    "\n",
    "unet_model_path = f\"{BASE_PATH}/models/unet/{organelle}/best_model_context.p\"\n",
    "mg_model_path = f\"{BASE_PATH}/models/mg/{organelle}/model_context.pt\"\n",
    "data_path = f\"{BASE_PATH}/data/{organelle}/cells\"\n",
    "test_csv_path = f\"{BASE_PATH}/data/{organelle}/metadata/test_images.csv\"\n",
    "test_context_path = f\"{BASE_PATH}/data/{organelle}/metadata/test_context.csv\"\n",
    "validation_csv_path = f\"{BASE_PATH}/data/{organelle}/metadata/valid_images.csv\"\n",
    "validation_context_path = f\"{BASE_PATH}/data/{organelle}/metadata/valid_context.csv\"\n",
    "patch_size = (32, 64, 64, 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- Load context config ---\n",
    "with open(f\"{BASE_PATH}/models/unet/{organelle}/model_config.json\", 'r') as file:\n",
    "    context_model_config = json.load(file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transforms_config = context_model_config[\"transforms\"]\n",
    "\n",
    "# Evaluate each string in the config using `eval`, injecting train_patch_size\n",
    "transforms = {\n",
    "    k: eval(v, {\"normalize\": normalize,\n",
    "                \"normalize_with_mask\": normalize_with_mask,\n",
    "                \"Propper\": Propper,\n",
    "                \"train_patch_size\": patch_size[:-1]})\n",
    "    for k, v in transforms_config.items()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# === Load Datasets ===\n",
    "train_dataset = CELTICDataGen(train_csv_path, data_path, train_context_path, transforms, signals_are_masked)\n",
    "val_dataset = CELTICDataGen(validation_csv_path, data_path, validation_context_path, transforms, signals_are_masked)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get a batch from the train loader\n",
    "(sample_batch, target_batch, mask) = next(iter(train_loader))\n",
    "\n",
    "# Unpack signal and context\n",
    "signal, context_ex = sample_batch\n",
    "\n",
    "# # Print shapes\n",
    "print(\"Signal shape:\", signal.shape)         # Expected: (1, C, D, H, W)\n",
    "print(\"Target shape:\", target_batch.shape)   # Expected: (1, C, D, H, W)\n",
    "print(\"Context shape:\", context_ex.shape)       # Expected: (1, num_features)\n",
    "\n",
    "# Optionally: visualize a slice of the volume (e.g., middle slice in depth)\n",
    "\n",
    "middle_slice = signal.shape[2] // 2\n",
    "plt.imshow(signal[0, 0, middle_slice].cpu().numpy(), cmap='viridis')\n",
    "plt.title(\"Middle slice of signal volume\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "context_features = context_model_config['context_features']\n",
    "daft_embedding_factor = context_model_config['daft_embedding_factor']\n",
    "daft_scale_activation = context_model_config['daft_scale_activation']\n",
    "\n",
    "context_df = pd.read_csv(train_context_path)\n",
    "context = {\n",
    "    'context_features': context_features,\n",
    "    'context_features_len': context_df.shape[1],\n",
    "    'daft_embedding_factor': daft_embedding_factor,\n",
    "    'daft_scale_activation': daft_scale_activation\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# --- Load CELTIC model ---\n",
    "celtic_model = CELTICModel(context=context, signals_are_masked=signals_are_masked)\n",
    "celtic_model.load_state(unet_model_path)\n",
    "celtic_wrapper = CELTICWrapper(celtic_model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Assuming these are already loaded:\n",
    "# signal: torch.Tensor of shape (1, 1, D, H, W)\n",
    "# context: torch.Tensor of shape (1, F)\n",
    "\n",
    "# Optionally move to the same device as the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "celtic_model.net.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "signal = signal.to(device)\n",
    "context_ex = context_ex.to(device)\n",
    "mask = mask.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run prediction using wrapper (batch size 1)\n",
    "\n",
    "# signal_np = signal.squeeze(0).numpy()   # (1, D, H, W)\n",
    "# context_np = context_ex.squeeze(0).numpy()  # (F,)\n",
    "\n",
    "# prediction = celtic_wrapper(signal_np, context_np)\n",
    "prediction = celtic_wrapper(signal, context_ex, mask)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(prediction.cpu()[0,0,16], cmap='viridis')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(target_batch[0,0,16], cmap='viridis')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# === Initialize Models ===\n",
    "unet = UNet3D(in_channels=2, out_channels=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mg = MaskGenerator(patch_size, unet, celtic_wrapper, mask_loss_weight=0.95, weighted_pcc=weighted_pcc,\n",
    "                   pcc_target=0.92)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if CONTINUE_TRAINING and os.path.exists(mg_model_path):\n",
    "    mg.load_state_dict(torch.load(mg_model_path, map_location=device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8c6ed6e-de91-4156-962b-0cee2990fb09",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# === Training Setup ===\n",
    "mg.to(device)\n",
    "num_epochs = 100\n",
    "optimizer = optim.Adam(mg.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b43c0aee-8722-4291-901a-cda0c0a5fda9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=True, path=f\"{mg_model_path}\"):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] No improvement for {self.counter} epoch(s).\")\n",
    "            if self.counter >= self.patience:\n",
    "                if self.verbose:\n",
    "                    print(\"[EarlyStopping] Stopping training.\")\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        if self.verbose:\n",
    "            print(f\"[EarlyStopping] Validation loss improved. Saving model to {self.path}\")\n",
    "        torch.save(model.state_dict(), self.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9abc016-2b75-4fef-a0aa-c0bb682c4373",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mw = mg.mask_loss_weight\n",
    "lr = optimizer.param_groups[0]['lr']\n",
    "early_stopper = EarlyStopping(path=f\"{mg_model_path[:-3]}_mw_{mw}_lr_{lr}.pt\")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    mg.train()\n",
    "    for (x, context), y, m in train_loader:\n",
    "        x, y, context, m = x.to(device), y.to(device), context.to(device), m.to(device)\n",
    "        loss_dict = mg.train_step((x, context), y, m, optimizer)\n",
    "    print(f\"Epoch {epoch}: {loss_dict}\")\n",
    "\n",
    "    mg.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for (x, context), y, m in val_loader:\n",
    "            x, y, context, m = x.to(device), y.to(device), context.to(device), m.to(device)\n",
    "            val_loss_dict = mg.test_step((x, context), y, m)\n",
    "            val_losses.append(val_loss_dict[\"val_loss\"])\n",
    "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "    print(f\"Validation: {val_loss_dict}\")\n",
    "\n",
    "    # Maybe use single batch and not average\n",
    "    early_stopper(avg_val_loss, mg)\n",
    "    if early_stopper.early_stop:\n",
    "        break\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd22aa-4f4d-41f4-a31a-36bb9b84be13",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (single_cell)",
   "language": "python",
   "name": "single_cell"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}