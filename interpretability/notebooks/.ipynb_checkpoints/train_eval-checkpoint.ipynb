{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b1f1b8-a3a1-4227-be5b-067318232b06",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d1a66a3-b92b-4a8d-9bcb-b0094b9a7df2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from collections import Counter, deque\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import itertools\n",
    "import math\n",
    "import tifffile as tiff\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "044101a0-8f65-4847-a0cf-5409869393b1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da03a0c5-58fb-4bcc-af2f-751c4d3d01f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\gadmi\\\\Desktop\\\\University Files\\\\Meitar\\\\Thesis\\\\Paper\\\\GitHub\\\\repo\\\\interpretability'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define base path for all operations\n",
    "BASE_PATH = os.path.dirname(os.getcwd())\n",
    "BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a5da671-d843-4890-a191-41c395c649f1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_organelles = [\"Nucleolus-(Granular-Component)\", \"Nuclear-envelope\", \"Mitochondria\", \"Actin-filaments\",\n",
    "                 \"Endoplasmic-reticulum\", \"Plasma-membrane\", \"Microtubules\", \"DNA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a562c74d-58eb-4eb8-9c1c-6f7cd6657585",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "organelle = all_organelles[1]\n",
    "unet_model_path = f\"{BASE_PATH}/models/unet/{organelle}/\"\n",
    "mg_model_path = f\"{BASE_PATH}/models/mg/{organelle}/\"\n",
    "train_csv_path = f\"{BASE_PATH}/data/{organelle}/image_list_train.csv\"\n",
    "test_csv_path = f\"{BASE_PATH}/data/{organelle}/image_list_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c754b56-cc47-4a01-bc9f-aa0a674084be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_channel=0\n",
    "if organelle == \"DNA\":\n",
    "    target_channel=1\n",
    "else:\n",
    "    target_channel=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "482d18b6-6ebf-4355-8884-77f6bb8903ea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "# Load neccessary models\n",
    "unet = load_model(unet_model_path)\n",
    "mg = load_model(mg_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5675718e-c92f-40b0-be31-cc7a2f7dbcf4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ba6f4-ade1-4d03-ade3-871963810d80",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e04f3df-08b1-4fee-81f4-a3ff85d03373",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tf_pearson_corr_aux(x,y):\n",
    "    mean_x = tf.reduce_mean(x)\n",
    "    mean_y = tf.reduce_mean(y)\n",
    "    std_x = tf.math.reduce_std(x-mean_x)\n",
    "    std_y = tf.math.reduce_std(y-mean_y)\n",
    "    cc = tf.reduce_mean((x - mean_x) * (y - mean_y)) / (std_x * std_y)\n",
    "    return cc\n",
    "\n",
    "def tf_pearson_corr(y_true, y_pred, weights=None):    \n",
    "    if weights is not None:\n",
    "        ind = tf.where(tf.logical_or(tf.reshape(weights,[-1])==1.0,tf.reshape(weights,[-1])==255.0))\n",
    "        non_ind = tf.where(tf.logical_and(tf.reshape(weights,[-1])!=1.0,tf.reshape(weights,[-1])!=255.0))    \n",
    "        cc = 0\n",
    "        t_weights = [1.0,0.0]\n",
    "        t = [ind,non_ind]\n",
    "        for i in range(2):\n",
    "            [x,y] = tf.cond(tf.shape(ind)[0]>0, lambda: [tf.gather(tf.reshape(y_true,[-1]), t[i]),tf.gather(tf.reshape(y_pred,[-1]), t[i])], lambda: [y_true,y_pred])       \n",
    "            cc = cc + t_weights[i]*tf_pearson_corr_aux(x,y)\n",
    "    else:\n",
    "        x = y_true\n",
    "        y = y_pred\n",
    "        cc = tf_pearson_corr_aux(x,y)\n",
    "    return cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7834351f-1967-4919-8cf2-d29b7835ae94",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def get_random_patch(image, gt, patch_size=(32, 128, 128)):\n",
    "    # Ensure the images are large enough for the patch size\n",
    "    assert image.shape[0] >= patch_size[0], \"Patch size is too large for the given z-dimension.\"\n",
    "    assert image.shape[1] >= patch_size[1] and image.shape[2] >= patch_size[2], \"Patch size is too large for the given image dimensions.\"\n",
    "    \n",
    "    # Calculate the maximum starting points for the random patch along each axis\n",
    "    max_z = image.shape[0] - patch_size[0]  # z-axis (depth)\n",
    "    max_x = image.shape[1] - patch_size[1]  # x-axis (height)\n",
    "    max_y = image.shape[2] - patch_size[2]  # y-axis (width)\n",
    "    \n",
    "    # Generate random start points along each axis\n",
    "    start_z = np.random.randint(0, max_z)\n",
    "    start_x = np.random.randint(0, max_x)\n",
    "    start_y = np.random.randint(0, max_y)\n",
    "\n",
    "    # Extract the 3D patch from the image and the ground truth (gt)\n",
    "    patch = image[start_z:start_z + patch_size[0], start_x:start_x + patch_size[1], start_y:start_y + patch_size[2]]\n",
    "    gt_patch = gt[start_z:start_z + patch_size[0], start_x:start_x + patch_size[1], start_y:start_y + patch_size[2]]\n",
    "    \n",
    "    return patch, gt_patch\n",
    "\n",
    "def get_patch(image, gt, sx, sy, patch_size=(32, 128, 128)):\n",
    "    # Ensure the images are large enough for the patch size\n",
    "    assert image.shape[0] >= patch_size[0], \"Patch size is too large for the given z-dimension.\"\n",
    "    assert image.shape[1] >= patch_size[1] and image.shape[2] >= patch_size[2], \"Patch size is too large for the given image dimensions.\"\n",
    "\n",
    "    # Extract the 3D patch from the image and the ground truth (gt)\n",
    "    patch = image[16:48, sx:sx + patch_size[1], sy:sy + patch_size[2]]\n",
    "    gt_patch = gt[16:48, sx:sx + patch_size[1], sy:sy + patch_size[2]]\n",
    "    \n",
    "    return patch, gt_patch\n",
    "\n",
    "def calculate_iou(i1, i2):\n",
    "    tp = np.logical_and(i1, i2)\n",
    "    fp = np.subtract(np.logical_or(i1, i2), i1)\n",
    "    fn = np.subtract(np.logical_or(i1, i2), i2)\n",
    "    if float((tp.sum() + fp.sum() + fn.sum())) == 0:\n",
    "        return 0\n",
    "    iou_score = float(tp.sum()) / float((tp.sum() + fp.sum() + fn.sum()))\n",
    "    return float(iou_score)\n",
    "\n",
    "def calculate_pcc(image1, image2):\n",
    "    if image1.shape != image2.shape:\n",
    "        raise ValueError(\"Images must have the same dimensions for correlation calculation.\")\n",
    "    \n",
    "    # Flatten the images to 1D arrays\n",
    "    image1_flat = image1.ravel()\n",
    "    image2_flat = image2.ravel()\n",
    "    \n",
    "    # Compute the Pearson correlation coefficient matrix\n",
    "    correlation_matrix = np.corrcoef(image1_flat, image2_flat)\n",
    "    \n",
    "    # The Pearson correlation coefficient is in position [0, 1] of the matrix\n",
    "    pcc = correlation_matrix[0, 1]\n",
    "    return pcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a12ec3d2-d1f2-4858-bc88-f17221e0745f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create fixed indices for test and validation\n",
    "indices = {}\n",
    "i = 0\n",
    "for x in range(0, 496, 96):\n",
    "    for y in range(0, 796, 96):\n",
    "        indices[i] = (x,y)\n",
    "        i += 1\n",
    "\n",
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f7b6792-4c98-495e-aee8-04bbe36a73aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Transformation functions-make into a class\n",
    "def normalize_std(input_image):\n",
    "    # Ensure input is a numpy array and convert to float64 for precision\n",
    "    input_image = np.array(input_image, dtype=np.float64)\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    mean = np.mean(input_image)\n",
    "    std = np.std(input_image)\n",
    "\n",
    "    if (np.isnan(mean) or np.isnan(std) or np.isinf(mean) or np.isinf(std)):\n",
    "        max_val = np.max(input_image[np.isfinite(input_image)])\n",
    "        input_image = np.where(input_image==np.inf, max_val, input_image)\n",
    "        mean = np.mean(input_image,dtype=np.float64)\n",
    "        std = np.std(input_image,dtype=np.float64)\n",
    "    \n",
    "    # Check and adjust standard deviation to avoid division by zero\n",
    "    if std == 0:\n",
    "        std = 1  # Prevent division by zero; alternatively could use a very small number\n",
    "    \n",
    "    # Normalize the image\n",
    "    normalized_image = (input_image - mean) / std\n",
    "    \n",
    "    # Replace NaN values that might result from zero divisions or infinite values in input\n",
    "    normalized_image = np.nan_to_num(normalized_image, nan=0.0)\n",
    "    \n",
    "    return normalized_image\n",
    "    \n",
    "def normalize_other(image_ndarray,max_value=255,dtype=np.uint8) -> np.ndarray:\n",
    "    image_ndarray = image_ndarray.astype(np.float64)\n",
    "    max_var = np.max(image_ndarray!=np.inf)\n",
    "    image_ndarray = np.where(image_ndarray==np.inf,max_var,image_ndarray)\n",
    "    temp_image = image_ndarray-np.min(image_ndarray)\n",
    "    return ((temp_image)/((np.max(temp_image))*max_value)).astype(dtype)\n",
    "\n",
    "def slice_image(image_ndarray: np.ndarray, indexes: list) -> np.ndarray:\n",
    "    n_dim = len(image_ndarray.shape)\n",
    "    slices = [slice(None)] * n_dim\n",
    "    for i in range(len(indexes)):\n",
    "        slices[i] = slice(indexes[i][0], indexes[i][1])\n",
    "    slices = tuple(slices)\n",
    "    sliced_image = image_ndarray[slices]\n",
    "    return sliced_image\n",
    "\n",
    "def mask_image_func(image_ndarray, mask_template_ndarray) -> np.ndarray:\n",
    "    mask_ndarray = mask_template_ndarray\n",
    "    return np.where(mask_ndarray == 255, image_ndarray, np.zeros(image_ndarray.shape))\n",
    "\n",
    "def resize_image(patch_size, image):\n",
    "    # only donwsampling, so use nearest neighbor that is faster to run\n",
    "    resized_image = np.zeros(patch_size)\n",
    "    for i in range(image.shape[0]):\n",
    "        resized_image[i] = tf.image.resize(\n",
    "            image[i], (patch_size[1], patch_size[2]\n",
    "                       ), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "        )\n",
    "    # resized_image = tf.cast(resized_image, tf.float16)  # / 127.5 - 1.0\n",
    "    return resized_image\n",
    "\n",
    "def augment_image(image_ndarray):\n",
    "    image = np.rot90(image_ndarray, axes=(2, 3), k=np.random.random_integers(0, 3))\n",
    "    return image\n",
    "\n",
    "def dilate_image(image):\n",
    "    for h in range(image.shape[1]):\n",
    "        image[0, h, :, :] = cv2.dilate(image[0, h, :, :].astype(np.uint8), self.dilate_kernel)   \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c06a1888-b0b4-4bfc-bb4d-b2e9e89c88a8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Transformations\n",
    "class Transpose:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        image = image.transpose(0, 4, 1, 2, 3) # PyTorch expects the input tensor format for CNNs as (batch_size, channels, height, width).\n",
    "        return image\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    # transforms.RandomHorizontalFlip(), # Flip the image horizontally\n",
    "    # transforms.RandomRotation(degrees=10), # Rotate the image by up to 10 degrees\n",
    "    # Transpose(),\n",
    "    transforms.ToTensor(), # Convert to a tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize values\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    # Transpose(),\n",
    "    transforms.ToTensor(), # Convert to a tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize values\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090bda8c-bf47-4fd8-8b57-61cb871882f6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "54f50fe8-90e3-46df-9dd5-4ef01ab02186",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RegressionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class inherits from pytorch Dataset and defines basic functions that are\n",
    "    needed for using pycharm operations on our data.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path, transform=None, min_=0.0, max_=1.0):\n",
    "        self.df = pd.read_csv(csv_path) # CSV with image locations\n",
    "        self.transform = transform # list of transformations on the data\n",
    "        self.min_ = min_\n",
    "        self.max_ = max_\n",
    "        self.patches_from_image = 32\n",
    "\n",
    "    def __len__(self):\n",
    "        # Total patches for selected images\n",
    "        num_selected_images = math.ceil((self.max_ - self.min_) * len(self.df))\n",
    "        return num_selected_images * self.patches_from_image\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index >= len(self):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "\n",
    "        num_selected_images = math.ceil((self.max_ - self.min_) * len(self.df))\n",
    "        start_idx = math.floor(len(self.df) * self.min_)\n",
    "        image_idx = start_idx + (index // self.patches_from_image) % num_selected_images\n",
    "        path = self.df.loc[image_idx, 'path_tiff']\n",
    "        path = path.replace('Interpretability/interpretability', \"\")\n",
    "        path = BASE_PATH + '/' + path.split('//')[1]\n",
    "        image = np.array(tiff.imread(path))\n",
    "        input_image = image[input_channel, :, :, :]\n",
    "        target_image = image[target_channel, :, :, :]\n",
    "\n",
    "        input_image = normalize_std(input_image)\n",
    "\n",
    "        input_patch, target_patch = get_random_patch(input_image, target_image)\n",
    "\n",
    "        input_patch = np.expand_dims(input_patch, axis=-1)\n",
    "        input_patch = np.expand_dims(input_patch, axis=0)\n",
    "        target_patch = np.expand_dims(target_patch, axis=-1)\n",
    "        target_patch = np.expand_dims(target_patch, axis=0)\n",
    "\n",
    "        target_prediction = unet(input_patch)\n",
    "        mask_prediction = mg.generator([input_patch, target_prediction]) # Check if casting is needed\n",
    "        error_rate = 1 - abs(tf_pearson_corr(target_prediction, tf.cast(target_patch, tf.float64)))\n",
    "        error_rate = tf.where(tf.math.is_nan(error_rate), tf.zeros_like(error_rate), error_rate)\n",
    "\n",
    "        mask_prediction = mask_prediction.numpy()\n",
    "        combined_input = np.concatenate([target_prediction, mask_prediction], axis=-1)\n",
    "\n",
    "        combined_input = torch.from_numpy(combined_input).float()\n",
    "        combined_input = combined_input.permute(0, 4, 1, 2, 3)\n",
    "        combined_input = combined_input.squeeze(0)\n",
    "        error_rate = torch.tensor([error_rate.numpy()], dtype=torch.float32)\n",
    "\n",
    "        return combined_input, error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "84dda007-abe0-4e57-932c-3c8e8fd151a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RegressionTestDataset(Dataset):\n",
    "        \"\"\"\n",
    "        This class inherits from pytorch Dataset and defines basic functions that are\n",
    "        needed for using pycharm operations on our data.\n",
    "        \"\"\"\n",
    "        def __init__(self, csv_path, indices, transform=None, min_=0.0, max_=1.0):\n",
    "            self.df = pd.read_csv(csv_path) # CSV with image locations\n",
    "            self.indices = indices\n",
    "            self.transform = transform # list of transformations on the data\n",
    "            self.min_ = min_\n",
    "            self.max_ = max_\n",
    "            self.patches_from_image = 54 # Patches from test image\n",
    "\n",
    "        def __len__(self):\n",
    "            # Total patches for selected images\n",
    "            num_selected_images = math.ceil((self.max_ - self.min_) * len(self.df))\n",
    "            return num_selected_images * self.patches_from_image\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            if index >= len(self):\n",
    "                raise IndexError(\"Index out of range\")\n",
    "\n",
    "            # Calculate which image and patch to access directly\n",
    "            num_selected_images = math.ceil((self.max_ - self.min_) * len(self.df))\n",
    "            start_idx = math.floor(len(self.df) * self.min_)\n",
    "\n",
    "            # Compute image index and patch index from the given dataset index\n",
    "            image_idx = start_idx + (index // self.patches_from_image) % num_selected_images\n",
    "            patch_idx = index % self.patches_from_image\n",
    "\n",
    "            # Load the correct image\n",
    "            path = self.df.loc[image_idx, 'path_tiff']\n",
    "            path = path.replace('Interpretability/interpretability', \"\")\n",
    "            path = BASE_PATH + '/' + path.split('//')[1]\n",
    "            image = np.array(tiff.imread(path))\n",
    "            input_image = image[input_channel, :, :, :]\n",
    "            target_image = image[target_channel, :, :, :]\n",
    "\n",
    "            # Apply normalization\n",
    "            input_image = normalize_std(input_image)\n",
    "\n",
    "            # Access the specific patch\n",
    "            sx, sy = self.indices[patch_idx]  # Directly map to the correct indices\n",
    "            input_patch, target_patch = get_patch(input_image, target_image, sx, sy)\n",
    "\n",
    "            input_patch = np.expand_dims(input_patch, axis=-1)\n",
    "            input_patch = np.expand_dims(input_patch, axis=0)\n",
    "            target_patch = np.expand_dims(target_patch, axis=-1)\n",
    "            target_patch = np.expand_dims(target_patch, axis=0)\n",
    "\n",
    "            target_prediction = unet(input_patch)\n",
    "            mask_prediction = mg.generator([input_patch, target_prediction]) # Check if casting is needed\n",
    "            error_rate = 1 - abs(tf_pearson_corr(target_prediction, tf.cast(target_patch, tf.float64)))\n",
    "            error_rate = tf.where(tf.math.is_nan(error_rate), tf.zeros_like(error_rate), error_rate)\n",
    "\n",
    "            mask_prediction = mask_prediction.numpy()\n",
    "            combined_input = np.concatenate([target_prediction, mask_prediction], axis=-1)\n",
    "\n",
    "            combined_input = torch.from_numpy(combined_input).float()\n",
    "            combined_input = combined_input.permute(0, 4, 1, 2, 3)\n",
    "            combined_input = combined_input.squeeze(0)\n",
    "            error_rate = torch.tensor([error_rate.numpy()], dtype=torch.float32)\n",
    "\n",
    "            return combined_input, error_rate, input_patch, target_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750465e8-f1ef-4f43-8155-2887980cba72",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f2fb7d38-79e5-4dfb-90e9-cd79460e7dbf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.models.video import r3d_18\n",
    "\n",
    "class ResNet3DRegression(nn.Module):\n",
    "    def __init__(self, fine_tune_layers='partial'):\n",
    "        super(ResNet3DRegression, self).__init__()\n",
    "        # Load pretrained 3D ResNet\n",
    "        self.resnet3d = r3d_18(pretrained=True)\n",
    "        \n",
    "        # Adjust the first convolutional layer for single-channel input\n",
    "        self.resnet3d.stem[0] = nn.Conv3d(\n",
    "            in_channels=2,\n",
    "            out_channels=64,\n",
    "            kernel_size=(3, 7, 7),\n",
    "            stride=(1, 2, 2),\n",
    "            padding=(1, 3, 3),\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        self.resnet3d.fc = nn.Identity()  # Remove the classification head\n",
    "\n",
    "        # Fully connected layers for regression\n",
    "        self.fc1 = nn.Linear(512, 128)  # ResNet3D outputs 512 features\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "        # Fine-tuning options\n",
    "        if fine_tune_layers == 'fc_only':  # Train only fc1 and fc2\n",
    "            for param in self.resnet3d.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif fine_tune_layers == 'partial':  # Train fc1, fc2, and later layers (e.g., layer4)\n",
    "            for name, param in self.resnet3d.named_parameters():\n",
    "                if 'layer4' in name or 'fc' in name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        elif fine_tune_layers == 'full':  # Train all layers\n",
    "            for param in self.resnet3d.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet3d(x)  # Pass through 3D ResNet\n",
    "        x = torch.relu(self.fc1(x))  # Fully connected layer 1\n",
    "        # x = torch.nn.functional.softplus(self.fc2(x))  # Output layer\n",
    "        x = torch.relu(self.fc2(x))  # Output layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fef5e3-5d1a-4a9d-ab2e-05b0351c0f8f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0a29cfd3-26de-466f-b941-93353e23beac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rates = [1e-5]\n",
    "batch_sizes = [16] # 8, 16, 32, 64\n",
    "# momentums = [0.5, 0.7, 0.9]\n",
    "weight_decays = [0.01]\n",
    "optimizers = []\n",
    "use_batch_norms = [True]\n",
    "use_dropouts = [False]\n",
    "\n",
    "# Trying two optimizers\n",
    "for lr in learning_rates:\n",
    "    for wd in weight_decays:\n",
    "        optimizers.append(('adam',lr,wd))\n",
    "    # for momentum in momentums:\n",
    "    #     optimizers.append(('sgd',lr,momentum))\n",
    "\n",
    "# grid_search_params = list(itertools.product(learning_rates, batch_sizes, optimizers, use_batch_norms, use_dropouts))\n",
    "grid_search_params = list(itertools.product(batch_sizes, optimizers, use_batch_norms, use_dropouts))\n",
    "\n",
    "len(grid_search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1fe248ef-4ad3-4830-82ea-221285435fab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length in patches: 128\n",
      "Validation data length in patches: 54\n",
      "Test data length in patches: 54\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_data = RegressionDataset(train_csv_path, transform=train_transforms, max_=0.9)\n",
    "val_data = RegressionTestDataset(train_csv_path, transform=test_transforms, indices=indices, min_=0.9)\n",
    "test_data = RegressionTestDataset(test_csv_path, transform=test_transforms, indices=indices)\n",
    "\n",
    "# Data size\n",
    "print(f\"Train data length in patches: {len(train_data)}\")\n",
    "print(f\"Validation data length in patches: {len(val_data)}\")\n",
    "print(f\"Test data length in patches: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd77e8-01d5-4fc5-be32-4f1273d88059",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU is not available\")\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU if GPU is not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373c510-deae-451a-abe0-b9feb1560a30",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_costs(path, costs, val_costs):\n",
    "    \"\"\"\n",
    "    Plotting train and validation losses through training\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.plot(costs, label='Training Cost')\n",
    "    plt.plot(val_costs, label='Validation Cost')\n",
    "\n",
    "    plt.xlabel('Iteration (per epochs)')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Train cost reduction over iterations')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.savefig(f'{path}/costs_plot.pdf')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "number_of_epochs = 100 # 200\n",
    "patience = 20  # Number of epochs to wait for improvement\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "top_5_loss = []\n",
    "model = None\n",
    "\n",
    "for params in grid_search_params:\n",
    "    # lr, batch_size, optimizer, use_batchnorm, use_dropout = params\n",
    "    batch_size, optimizer, use_batchnorm, use_dropout = params\n",
    "    lr = optimizer[1]\n",
    "\n",
    "    if optimizer[0] == 'adam':\n",
    "        params_text = f\"best_lr_{lr}_batch_size_{batch_size}_optimizer_{optimizer[0]}_weight_decay_{optimizer[2]}_use_batchnorm_{use_batchnorm}_use_dropout_{use_dropout}\"\n",
    "    elif optimizer[0] == 'sgd':\n",
    "        params_text = f\"best_lr_{lr}_batch_size_{batch_size}_optimizer_{optimizer[0]}_momentum_{optimizer[2]}_use_batchnorm_{use_batchnorm}_use_dropout_{use_dropout}\"\n",
    "\n",
    "    path = f\"{BASE_PATH}/models/confidence/{organelle}/{params_text}\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    else:\n",
    "        print(path)\n",
    "        continue\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    total_epochs = 0\n",
    "    output_txt = ''\n",
    "\n",
    "    # Using DataLoader for training with the data in Pytorch models\n",
    "    train_dataLoader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    validation_dataLoader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "    print(\"Finished with dataLoaders\")\n",
    "\n",
    "    # Create model\n",
    "    model = ResNet3DRegression()\n",
    "    print(\"Model created\")\n",
    "\n",
    "    # Load existing model if necessary\n",
    "    # model = ResNet3DRegression(use_batchnorm=use_batchnorm, use_dropout=use_dropout)\n",
    "    # model.load_state_dict(torch.load(f\"{BASE_PATH}/ISL/experiments_Mask_Pred/{organelle}/{params_text}/model.pt\", weights_only=True))\n",
    "    # print(\"Model loaded\")\n",
    "\n",
    "    if optimizer[0] == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=optimizer[1], momentum=optimizer[2])\n",
    "    elif optimizer[0] == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=optimizer[1], weight_decay=optimizer[2])\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    convergence_time = time.time()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(number_of_epochs):\n",
    "        print(f\"Starting epoch {total_epochs+1}\")\n",
    "        total_epochs += 1\n",
    "        time_start = time.time()\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Train\n",
    "        for batch_id, (img, error_rate) in enumerate(train_dataLoader, 1):\n",
    "            # (img, error_rate) = (img.to(device), error_rate.to(device))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            output = model(img)\n",
    "            # output = output.to(device)\n",
    "            loss = loss_fn(output, error_rate)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"Batch {batch_id} finished, loss equals {loss.item()}\")\n",
    "            if batch_id % 1 == 0:\n",
    "                print(f\"output is {output}, error is {error_rate}\")\n",
    "\n",
    "        # Calculate train loss\n",
    "        avg_train_loss = total_loss / len(train_dataLoader)  # Average loss for the epoch\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validate\n",
    "        print(\"Starting validation\")\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for img, error_rate, ip, tp in validation_dataLoader:\n",
    "                # (img, error_rate) = (img.to(device), error_rate.to(device))\n",
    "                output = model(img)\n",
    "                loss = loss_fn(output, error_rate)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        # Calculate validation loss\n",
    "        avg_val_loss = total_val_loss / len(validation_dataLoader)  # Average validation loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{number_of_epochs}, Time: {time.time() - time_start:.2f} seconds, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "        output_txt += f\"Epoch {epoch + 1}/{number_of_epochs}, Time: {time.time() - time_start:.2f} seconds, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\\n\"\n",
    "\n",
    "        # Save at end of epoch\n",
    "        torch.save(model.state_dict(), f\"{path}/model.pt\")\n",
    "        print(\"model saved\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after epoch {epoch + 1}\")\n",
    "            output_txt += f\"Early stopping triggered after epoch {epoch + 1}\\n\"\n",
    "            torch.save(model.state_dict(), f'{path}/model.pt')\n",
    "            print(\"model saved\")\n",
    "            break\n",
    "\n",
    "        # Decay learning rate\n",
    "        lr = lr * 0.99\n",
    "\n",
    "\n",
    "    convergence_time = time.time() - convergence_time\n",
    "\n",
    "    plot_costs(path, train_losses, val_losses)\n",
    "\n",
    "\n",
    "    ### TEST ###\n",
    "    test_dataLoader = DataLoader(test_data, batch_size=16, shuffle=True)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss_val = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, error_rate, ip, tp in test_dataLoader:\n",
    "            # (img, error_rate) = (img.to(device), error_rate.to(device))\n",
    "            output = model(img)\n",
    "            loss = loss_fn(output, error_rate)\n",
    "            test_loss_val += loss.item()\n",
    "\n",
    "    avg_test_loss = test_loss_val / len(test_dataLoader)  # Average test loss\n",
    "\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "    output_txt += f\"Test Loss: {avg_test_loss:.4f}\\n\"\n",
    "\n",
    "\n",
    "    meta_data = {'convergence_time': convergence_time,\n",
    "                 'number_of_epochs': total_epochs,\n",
    "                 'final_test_loss': avg_test_loss,\n",
    "                 'final_train_loss': train_losses[-1],\n",
    "                 'train_losses': train_losses,\n",
    "                 'params': params_text,\n",
    "                 'final_validation_loss': val_losses[-1],\n",
    "                 'validation_losses': val_losses,\n",
    "                 'output_txt': output_txt}\n",
    "\n",
    "    top_5_loss.append((avg_test_loss, params_text, meta_data))\n",
    "\n",
    "\n",
    "    with open(f'{path}/meta_data.json', 'w') as f:\n",
    "        f.write(json.dumps(meta_data, indent=4))\n",
    "    with open(f'{BASE_PATH}/models/confidence/{organelle}/current_top_5.json', 'w') as f:\n",
    "        f.write(json.dumps(sorted(top_5_loss,reverse=False, key=lambda x: x[0])[:5], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d962fed-de1f-4971-b408-6cb4f8e0a073",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86aca7c0-e764-41e3-abcc-4a0a116497a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for params in grid_search_params:\n",
    "    # lr, batch_size, optimizer, use_batchnorm, use_dropout = params\n",
    "    batch_size, optimizer, use_batchnorm, use_dropout = params\n",
    "    lr = optimizer[1]\n",
    "\n",
    "    if optimizer[0] == 'adam':\n",
    "        params_text = f\"best_lr_{lr}_batch_size_{batch_size}_optimizer_{optimizer[0]}_weight_decay_{optimizer[2]}_use_batchnorm_{use_batchnorm}_use_dropout_{use_dropout}\"\n",
    "    elif optimizer[0] == 'sgd':\n",
    "        params_text = f\"best_lr_{lr}_batch_size_{batch_size}_optimizer_{optimizer[0]}_momentum_{optimizer[2]}_use_batchnorm_{use_batchnorm}_use_dropout_{use_dropout}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1698cae-678e-4156-9cc6-65fcecf22d95",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = f\"{BASE_PATH}/models/confidence/{organelle}/{params_text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75b637c7-30f7-4984-9732-1011f1cb6237",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "mo = ResNet3DRegression(fine_tune_layers='partial')\n",
    "mo.load_state_dict(torch.load(f\"{path}/model.pt\", weights_only=True))\n",
    "mo.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd83e442-2f37-4a27-8d29-6571228689e3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Run Test\n",
    "\n",
    "error_predictions = []\n",
    "errors = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    img, err, ip, tp = test_data[i]\n",
    "    errors.append(err.numpy()[0])\n",
    "    img = torch.from_numpy(np.expand_dims(img, axis=0)).float()\n",
    "    pred = mo(img)\n",
    "    error_predictions.append(float(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e072d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = f\"{BASE_PATH}/variables\"\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944779ea-260d-450f-8978-5c91cfc97470",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save errors+error_predictions\n",
    "error_predictions_np = np.array(error_predictions)\n",
    "np.save(f\"{BASE_PATH}/variables/{organelle}_Error_Predictions.npy\", error_predictions_np)\n",
    "errors_np = np.array(errors)\n",
    "np.save(f\"{BASE_PATH}/variables/{organelle}_Errors.npy\", errors_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de4f77-f938-4a4f-b839-8fe4d61d2ead",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a03611c-79ef-4479-9d59-d335edbbdf6f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Confidence)",
   "language": "python",
   "name": "confidence"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
